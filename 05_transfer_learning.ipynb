{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning : Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import activations\n",
    "from keras import callbacks\n",
    "from keras import utils\n",
    "from keras import metrics\n",
    "from keras import applications\n",
    "\n",
    "\n",
    "from keras import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb  4 09:39:26 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650        Off | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   44C    P0               5W /  50W |      6MiB /  4096MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2030      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Get the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# # Download data\n",
    "# !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
    "\n",
    "# # Unzip the  downloaded file\n",
    "# zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\", \"r\")\n",
    "# zip_ref.extractall()\n",
    "# zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There'are 2 directories and 0 images in '10_food_classes_10_percent'\n",
      "There'are 10 directories and 0 images in '10_food_classes_10_percent/test'\n",
      "There'are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'\n",
      "There'are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'\n",
      "There'are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'\n",
      "There'are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'\n",
      "There'are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'\n",
      "There'are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'\n",
      "There'are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'\n",
      "There'are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'\n",
      "There'are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'\n",
      "There'are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'\n",
      "There'are 10 directories and 0 images in '10_food_classes_10_percent/train'\n",
      "There'are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'\n",
      "There'are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'\n",
      "There'are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'\n",
      "There'are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'\n",
      "There'are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'\n",
      "There'are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'\n",
      "There'are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'\n",
      "There'are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'\n",
      "There'are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'\n",
      "There'are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'\n"
     ]
    }
   ],
   "source": [
    "# How many images in each folder?\n",
    "import os\n",
    "\n",
    "# Walk through 10 percent data directories and list the number of files\n",
    "for dirpath, dirname, filename in os.walk(\"10_food_classes_10_percent\"):\n",
    "    print(f\"There'are {len(dirname)} directories and {len(filename)} images in '{dirpath}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Creating data loaders (preparing the data)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images:\n",
      "Found 750 images belonging to 10 classes.\n",
      "Testing Image:\n",
      "Found 2500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "Image_shape  = (224, 224)\n",
    "Batch_size = 32\n",
    "c_mode = \"categorical\"\n",
    "\n",
    "train_dir = \"10_food_classes_10_percent/train/\"\n",
    "test_dir = \"10_food_classes_10_percent/test/\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.0)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255.0)\n",
    "\n",
    "print(\"Training Images:\")\n",
    "train_data_10_percent = train_datagen.flow_from_directory(train_dir,\n",
    "                                                        target_size=Image_shape,\n",
    "                                                        batch_size=Batch_size,\n",
    "                                                        class_mode=c_mode)\n",
    "\n",
    "print(\"Testing Image:\")\n",
    "test_data = train_datagen.flow_from_directory(test_dir,\n",
    "                                            target_size=Image_shape,\n",
    "                                            batch_size=Batch_size,\n",
    "                                            class_mode=c_mode)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorBoard callback (functionized because need to create a new one for each model)\n",
    "\n",
    "import datetime\n",
    "\n",
    "def create_tensorboard_callback(dir_name, experiment_name):\n",
    "    log_dir = dir_name + \"/\" + experiment_name + \"/\"  + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = callbacks.TensorBoard(log_dir=log_dir)\n",
    "    print(f\"Saving Tensorboard log files to: {log_dir}\")\n",
    "    return tensorboard_callback\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet 50 v2 feature vector\n",
    "resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "\n",
    "# original: EfficientNet50 feature vector (version 1)\n",
    "efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n",
    "\n",
    "# New: EfficientNetB0 feature vector (version 2)\n",
    "efficientnet_url_new = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "def create_mode(model_url, num_classes=10):\n",
    "    \"\"\"\n",
    "    Take a tensorflow Hub URL and creates a Keras Sequential model with it\n",
    "    \n",
    "    Args: \n",
    "        model_url: A TensorflowHub feature extraction URL\n",
    "        num_classes: Number of output neurons in output layer,\n",
    "        should be equal to number of target classes, default 10,\n",
    "        \n",
    "    Returns:\n",
    "        An uncompiled Keras Sequential model with model_url as feature \n",
    "         extractor layer and Dense output layer with num_classes outputs \"\"\"\n",
    "    \n",
    "    # Download the pretrained model and save it as Keras Layer\n",
    "    feature_extractor_layer = hub.KerasLayer(model_url,\n",
    "                                             trainable=False,   # frozen the underlining patterns\n",
    "                                             name=\"feature_extraction_layer\",\n",
    "                                             input_shape= Image_shape+(3,)) \n",
    "    \n",
    "\n",
    "    # Create out own model\n",
    "    model = keras.Sequential([\n",
    "        feature_extractor_layer,\n",
    "        layers.Dense(num_classes, activation=activations.softmax, name=\"output_layer\")\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create model\n",
    "resnet_model = create_mode(resnet_url, num_classes=train_data_10_percent.num_classes)\n",
    "\n",
    "# Compile model\n",
    "resnet_model.compile(loss=losses.categorical_crossentropy,\n",
    "                     optimizer=optimizers.Adam(),\n",
    "                     metric=[metrics.categorical_accuracy])\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "resnet_history = resnet_model.fit(train_data_10_percent,\n",
    "                                  epochs=5,\n",
    "                                  steps_per_epoch=len(train_data_10_percent),\n",
    "                                  validation_data=test_data,\n",
    "                                  validation_steps=len(test_data),\n",
    "                                  callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",         # Save the experiment log here, directory\n",
    "                                                                         experiment_name=\"resnet50v2\")])    # Name of the experiment\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *EfficientNetV250*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after base_model: (None, 7, 7, 1280)\n",
      "Output shape after GlobalAveragePooling2D: (None, 1280)\n"
     ]
    }
   ],
   "source": [
    "# 1.Create base model with tf.keras.applications\n",
    "base_model = applications.EfficientNetV2B0(include_top=False)\n",
    "\n",
    "# 2. Freeze the base model (so the pre-learned patterns remain)\n",
    "base_model.trainable = False\n",
    "\n",
    "# 3. Create inputs into our model\n",
    "inputs = layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n",
    "\n",
    "# 4. If using ResNet50V2 you will need to normalize inputs (you don't have to for EfficientNet(s))\n",
    "# x = keras.preprocessing.Rescaling(1./255)(x)\n",
    "\n",
    "# 5. Pass the inputs to the base_model (note: using tf.keras.applications, EfficientNet inputs don't have to be normalized)\n",
    "x = base_model(inputs)\n",
    "\n",
    "# check the shape of the output of the base model\n",
    "print(\"Shape after base_model:\", x.shape) # x is the output of the EfficientNetB0 base model\n",
    "\n",
    "# 6. Average pool the outputs of the base model (aggregate all the most important information, reduce number of computations)\n",
    "x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "print(\"Output shape after GlobalAveragePooling2D:\", x.shape)\n",
    "\n",
    "# 7. Create the output activation layer\n",
    "outputs = layers.Dense(10, activation=activations.softmax, name=\"output_layer\")(x)\n",
    "\n",
    "# 8. Combine the inputs with the outputs into a model\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# 9. Compile the model\n",
    "model.compile(loss=losses.categorical_crossentropy,\n",
    "              optimizer=optimizers.Adam(),\n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1738637614.873580    8970 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "E0000 00:00:1738637615.967794    8970 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2025-02-04 09:53:36.017717: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at xla_ops.cc:577 : FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n",
      "2025-02-04 09:53:36.017763: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_8893/1092852901.py\", line 2, in <module>\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_iterator_46120]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 10. Fit the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m efficientnet_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_10_percent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_10_percent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;66;43;03m#  callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;66;43;03m#                                         experiment_name=\"efficientnetb0\")]\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_8893/1092852901.py\", line 2, in <module>\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/home/lovecrush/miniconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_iterator_46120]"
     ]
    }
   ],
   "source": [
    "# 10. Fit the model\n",
    "efficientnet_history = model.fit(train_data_10_percent,\n",
    "                                 epochs=5,\n",
    "                                 steps_per_epoch=len(train_data_10_percent),\n",
    "                                 validation_data=test_data,\n",
    "                                 validation_steps=int(0.25 * len(test_data)),\n",
    "                                #  callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
    "                                #                                         experiment_name=\"efficientnetb0\")]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
